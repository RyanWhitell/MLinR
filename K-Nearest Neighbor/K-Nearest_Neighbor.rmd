---
title: "K-Nearest Neighbor"
author: "Ryan Whitell"
output:
  html_document: default
csl: apa.csl
subtitle: Predicting Heart Disease in Patients
bibliography: bibliography.bibtex
---
```{r message=FALSE}
# Packages Required
library(ggplot2)  # Graphs
library(class)    # KNN
library(dummies)  # Dummy
library(gmodels)  # Cross Table
```

\  
\  

# Introduction
K-nearest neighbors is an extremely simple classification and regression algorithm that classifies or predicts a data point based on the majority vote of its nearest neighbors. The important characteristics of KNN are how many neighbors to consider (K) and the method used to calculate distance.

\  
\  

# The Heart Disease Data Set
This example uses the heart disease data set from @Detrano:nd, retrieved from the UCI machine learning repository [@Lichman:2013], to investigate and implement an example of KNN classification by predicting the presence of heart disease using 13 features collected from 303 patients:

  * age: age in years
  * sex: sex (1 = male; 0 = female)
  * cp: chest pain type
    + Value 1: typical angina
    + Value 2: atypical angina
    + Value 3: non-anginal pain
    + Value 4: asymptomatic
  * trestbps: resting blood pressure (in mm Hg on admission to the hospital)
  * chol: serum cholesterol in mg/dl
  * fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
  * restecg: resting electrocardiograph results
    + Value 0: normal
    + Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
    + Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
  * thalach: maximum heart rate achieved
  * exang: exercise induced angina (1 = yes; 0 = no)
  * oldpeak = ST depression induced by exercise relative to rest
  * slope: the slope of the peak exercise ST segment
    + Value 1: up sloping
    + Value 2: flat
    + Value 3: down sloping
  * ca: number of major vessels (0-3) colored by flourosopy
  * thal: 3 = normal; 6 = fixed defect; 7 = reversible defect

The 'num' label defines the presence of heart disease with numbers 0-4:
  * 1-4 meaning presence of heart disease
  * 0 meaning no presence of heart disease

\  
\  

# Exploratory Data Analysis
The purpose of exploring the data first is to get familiar with it and to see if anything is of interest.
```{r}
heart <- read.csv("processed.cleveland.data.csv") # Read in the data
```

```{r}
str(heart)
```

\  

#### age: age in years
```{r}
summary(heart$age)
```

```{r warning = FALSE, fig.cap="Figure 1: Age Distribution", fig.align='center'}
ggplot(heart, aes(heart$age)) + geom_histogram(binwidth = 1) + labs(x="Age (Years)", y="Count", title="age")
```

\  

#### sex: sex (1 = male; 0 = female)
```{r}
 table(heart$sex)
```

\  

#### cp: chest pain type
    + Value 1: typical angina
    + Value 2: atypical angina
    + Value 3: non-anginal pain
    + Value 4: asymptomatic
```{r}
table(heart$cp)
```

\  

#### trestbps: resting blood pressure (in mm Hg on admission to the hospital)
```{r}
summary(heart$trestbps)
```

```{r warning = FALSE, fig.cap="Figure 2: Resting Blood Pressure Distribution", fig.align='center'}
ggplot(heart, aes(heart$trestbps)) + geom_histogram(binwidth = 1) + labs(x="Resting Blood Pressure (mm Hg)", y="Count", title="trestbps")
```

\  

#### chol: serum cholestoral in mg/dl
```{r}
summary(heart$chol)
```

```{r warning = FALSE, fig.cap="Figure 3: Serum Cholestoral Distribution", fig.align='center'}
ggplot(heart, aes(heart$chol)) + geom_histogram(binwidth = 1) + labs(x="Serum Cholestoral (mg/dl)", y="Count", title="chol")
```

\  

#### fbs: (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false)
```{r}
table(heart$fbs)
```

\  

#### restecg: resting electrocardiographic results
    + Value 0: normal
    + Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)
    + Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria
```{r}
table(heart$restecg)
```

\  

#### thalach: maximum heart rate achieved
```{r}
summary(heart$thalach)
```

```{r warning = FALSE, fig.cap="Figure 4: Maximum Heart Rate Achieved Distribution", fig.align='center'}
ggplot(heart, aes(heart$thalach)) + geom_histogram(binwidth = 1) + labs(x="Hear Rate (b/min)", y="Count", title="thalach")
```

\  

#### exang: exercise induced angina (1 = yes; 0 = no)
```{r}
table(heart$exang)
```

\  

#### oldpeak = ST depression induced by exercise relative to rest
```{r}
summary(heart$oldpeak)
```

```{r warning = FALSE, fig.cap="Figure 5: ST Depression Induced by Exercise Relative to Rest Distribution", fig.align='center'}
ggplot(heart, aes(heart$oldpeak)) + geom_histogram(binwidth = 0.1) + labs(x="ST depression (mm)", y="Count", title="oldpeak")
```

\  

#### slope: the slope of the peak exercise ST segment
    + Value 1: up sloping
    + Value 2: flat
    + Value 3: down sloping
```{r}
table(heart$slope)
```

\  

#### ca: number of major vessels (0-3) colored by flourosopy
```{r}
table(heart$ca)
```

\  

#### thal: 3 = normal; 6 = fixed defect; 7 = reversible defect
```{r}
table(heart$thal)
```
\  

The importance of some of these variables can be defined by a domain expert, in this case a Doctor. This could help determine which variables can be omitted or how much each variable should be contributing to the classification. The data seems good to use except for some missing values.

\  
\  

# Data Preprocessing
### Handling Missing Values
Only categorical variables contain missing values. Imputing the missing values with the mean or median could be a good strategy. However, there are only a few so the entire rows can be removed without sacrificing too much data.
```{r}
heart[heart == "?"] <- NA # Replace occurrences of '?' with 'NA'
heart <- na.omit(heart)   # Omit rows with 'NA'
```

\  

### Separating the Label
```{r}
diagnosis <- heart$num                  # Save the classification column
heart <- subset(heart, select = -num)   # Remove it from the data set
diagnosis[diagnosis > 0] <- 1           # Set all true values to 1
```

\  

### Normalizing Data
The numerical variables have different ranges which skews the results of the KNN algorithms distance calculations. Normalizing the data will prevent larger scaled variables from dominating the outcome.
```{r}
# Returns a normalized vector
normalize <- function(x) {
  return ((x - min(x)) / (max(x) - min(x)))
}
```

```{r}
heart$age <- normalize(heart$age)
heart$trestbps <- normalize(heart$trestbps)
heart$chol <- normalize(heart$chol)
heart$thalach <- normalize(heart$thalach)
heart$oldpeak <- normalize(heart$oldpeak)
```

\  

### Dealing with Categorical Data
The categorical data shouldn't be used in a measure of distance so it must be converted to numerical data first using dummy variables. The heart disease data set has categorical data, like 'chest pain type', as a numerical attribute so it must be converted to a factor before the dummy.data.frame() function is run just for consistency.
```{r}
heart$sex <- as.factor(heart$sex)
heart$cp <- as.factor(heart$cp)
heart$fbs <- as.factor(heart$fbs)
heart$restecg <- as.factor(heart$restecg)
heart$exang <- as.factor(heart$exang)
heart$slope <- as.factor(heart$slope)
```

```{r}
heart <- dummy.data.frame(heart)  # Converts all factor variables into dummy variables and returns a data frame
```

\  
\  

# K-Nearest Neighbor

### Splitting the Data Between Testing and Training

The data is cleaned and pre-processed, it can now be used for KNN classification. To test our data, it is split between a test set and a training set.

```{r}
set.seed(77)                                                  # Get the same data each time
idx <- sample(2, nrow(heart), replace=TRUE, prob=c(0.7, 0.3)) # Create 2 Subsets with ratio 70:30
heart_train <- heart[idx==1, ]                                # Training subset
heart_test <- heart[idx==2, ]                                 # Testing subset
heart_train_diagnosis <- diagnosis[idx==1,]                   # Training labels
heart_test_diagnosis <- diagnosis[idx==2,]                    # Testing labels
```

\  

### Building the Classifier

```{r}
heart_test_predicions <- knn(train = heart_train, test = heart_test, cl = heart_train_diagnosis, k = 1)
```

\  

### Evaluating Performance

```{r}
# Returns the percentage of correct predictions
get.accuracy <- function(prediction, real) {
  accuracy <- prediction == real
  return (length(accuracy[accuracy == TRUE])/length(accuracy))
}
```

```{r}
get.accuracy(heart_test_predicions, heart_test_diagnosis)
```

\  
\  

### Improving the Performance

One way to improve KNN performance is to find the right value for K. For small data sets, looping over different values of K to find the best one is practical.

```{r}
# Returns the best K in range 1:max_k
get.k <- function(train, test, train.cl, test.cl, max_k) {
  # Aggregate results
  k <- c()
  a <- c()
  
  for (i in 1:max_k){
    # Run KNN
    prediction <- knn(train = train, test = test, cl = train.cl, k = i)
    
    # Evaluate
    accuracy <- get.accuracy(prediction, test.cl)
    
    # Aggregate results
    k <- c(k, i)
    a <- c(a, accuracy)
  }

  return (as.data.frame(list("K" = k, "Accuracy" = a)))
}
```

```{r}
results <- get.k(heart_train, heart_test, heart_train_diagnosis, heart_test_diagnosis, (length(heart_train$age)/2))
results[results$Accuracy == max(results$Accuracy),] # Find the best K
```

The best K to pick would be the lowest K with the highest accuracy. The lower K value is more efficient.

```{r fig.cap="Figure 6: Plot of Different K Values and Accuracy", fig.align='center'}
ggplot(data=results, aes(results$K, results$Accuracy)) + geom_point() + labs(x = "K", y = "Accuracy", title="How KNN Accuracy Differs with K")
```

\  
\  

### Cross Table

```{r}
CrossTable(x = heart_test_diagnosis, y = heart_test_predicions, prop.chisq=FALSE, dnn = c('predicted', 'actual'))
```

\  
\  

# Conclusion

With very little data wrangling or preprocessing, the KNN classifier was able to correctly predict the presence of heart disease with around 87% accuracy. With good data that has been processed correctly, KNN can be utilized to classify and predict future data points. KNN is a simple but very effective algorithm.

\  
\  

# References